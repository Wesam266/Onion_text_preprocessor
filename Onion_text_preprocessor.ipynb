{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import re\n",
    "import time\n",
    "from langdetect import detect\n",
    "from nltk import PorterStemmer\n",
    "from nltk import downloader\n",
    "from nltk import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OnionDir = 'D:/Wesam/Onion_Dataset/{0}/{0}.txt'\n",
    "\n",
    "class OnionTextPreprocessor(object):\n",
    "    \"\"\"\n",
    "    Class to pre-process the text sample\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "\n",
    "        self.FLAGS = re.MULTILINE | re.DOTALL\n",
    "        self.URL = ' <url>'\n",
    "        self.IMG = ' <image>'\n",
    "        self.NUMBER = ' <number> '\n",
    "        self.EMAIL = ' <email> '\n",
    "\n",
    "        with open('stops.txt', 'r', encoding='utf-8')as ins:\n",
    "            stop = ins.readlines()\n",
    "        stop = [PorterStemmer().stem_word(wrd.strip().lower()) for wrd in stop]\n",
    "        stop = list(set(stop))\n",
    "        self.stop = stop\n",
    "\n",
    "        # Download NLTK tokenizer package\n",
    "        try:\n",
    "            data.find(\"tokenizers/punkt\")\n",
    "        except LookupError:\n",
    "            downloader.download(\"punkt\")\n",
    "\n",
    "    def clean_onion(self, document, max_text_len=2000, min_text_len=5):\n",
    "        \"\"\"\n",
    "\n",
    "        :param document:\n",
    "        :param max_text_len: Maximum document length to cut ( to avoid large text)\n",
    "        :param min_text_len: Minimum document length to cut ( to avoid short text)\n",
    "        :return: cleaned onion document\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Decode the text from binary.\n",
    "        text_content = document#.decode(errors='replace').strip()\n",
    "\n",
    "        # Count the document words by space\n",
    "        text_list = text_content.split(' ')\n",
    "\n",
    "        # If the text length is more than max_text_len words, take only the first max_text_len words\n",
    "        if len(text_list) > max_text_len:\n",
    "            text_content = ' '.join(text_list[:max_text_len])\n",
    "\n",
    "        # Start cleaning the Text\n",
    "        text_content = [self.clean_text_with_reg(ln) for ln in text_content.split('\\n')]\n",
    "\n",
    "        # Remove Ref part in the text ( old scrap version)\n",
    "        if 'references' in text_content:\n",
    "            idx = text_content.index('references')\n",
    "            text_content = text_content[:idx]\n",
    "\n",
    "        # Remove duplicated lines\n",
    "        text_content = ' '.join(list(set(text_content)))\n",
    "\n",
    "        # Calculate the text hash\n",
    "        text_hash = hashlib.md5(text_content.encode()).hexdigest()\n",
    "\n",
    "        # Check if we still have text after the pre-processing (level_1)\n",
    "\n",
    "        if text_content:\n",
    "            # If the text is not empty, start pre-processing (Level_2)\n",
    "\n",
    "            # 1- Detect the language\n",
    "            lang = detect(text_content)\n",
    "\n",
    "            # 2- Stem text:\n",
    "            text_content = ' '.join([PorterStemmer().stem_word(word) for word in text_content.split()])\n",
    "\n",
    "            # 3- Remove stop words\n",
    "            text_content = re.sub(r'\\b(' + r'|'.join(self.stop) + r')\\b\\s*', ' ', text_content).strip()\n",
    "\n",
    "            # 4-  Remove extra space from the text:\n",
    "            text_content = re.sub(r'\\s+', ' ', text_content).strip()\n",
    "\n",
    "            # 5- Count the document words by space after cleaning it.\n",
    "            # If less than 5 words (after second level of preprocessing) , then set it to empty\n",
    "            text_list = text_content.split(' ')\n",
    "            if len(text_list) < min_text_len:\n",
    "                # If the text is empty, return empty text\n",
    "                text_content = ''\n",
    "                # By default, english\n",
    "                lang = 'en'\n",
    "        else:\n",
    "            # If the text is empty, return empty text\n",
    "            text_content = ''\n",
    "\n",
    "            # By default, english\n",
    "            lang = 'en'\n",
    "\n",
    "        spend_time = time.time() - start_time\n",
    "\n",
    "        return text_content\n",
    "\n",
    "    def clean_text_with_reg(self, text):\n",
    "        \"\"\"\n",
    "        This function prepossess the text before classifying it with regular expression.\n",
    "        :param text: input text of onion to clean\n",
    "        :return: cleaned text\n",
    "        \"\"\"\n",
    "        re_sub = lambda pattern, _: re.sub(pattern, _, text, flags=self.FLAGS)\n",
    "\n",
    "        # Remove PGP line:\n",
    "        text = re_sub(r'[^----]*(BEGIN|END) [^-----]*', ' ')\n",
    "\n",
    "        # Replace some S.C:\n",
    "        text = re_sub(r'[<>]', ' ')\n",
    "\n",
    "        # Replace Email:\n",
    "        text = re_sub(r'[\\w\\.-]+@[\\w\\.-]+', self.EMAIL)\n",
    "\n",
    "        # Replace some spacial chars.\n",
    "        text = re_sub('&', 'and')\n",
    "\n",
    "        # Replace all types or URLs\n",
    "        url_regex = r'^(?:http|ftp)s?://'\\\n",
    "        # domain name\n",
    "        r'(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\\.)+(?:[A-Z]{2,6}\\.?|[A-Z0-9-]{2,}\\.?)|'\\\n",
    "        # localhost...\n",
    "        r'localhost|'\\\n",
    "        # ...or ipv4\n",
    "        r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}|'\\\n",
    "        # ...or ipv6\n",
    "        r'\\[?[A-F0-9]*:[A-F0-9:]+\\]?)'\\\n",
    "        # optional port\n",
    "        r'(?::\\d+)?'\\\n",
    "        r'(?:/?|[/?]\\S+)$'\n",
    "        text = re_sub(url_regex, self.URL)\n",
    "\n",
    "        # Replace ordered list  [1] by <ol>\n",
    "        text = re_sub(r'(\\[)[\\d|\\w](\\])', ' ')  # ,self.OL)\n",
    "\n",
    "        # Replace @user by <user>\n",
    "        text = re_sub(r\"@\\w+\", ' ')  # , self.USER)\n",
    "\n",
    "        # Replace IMG:\n",
    "        text = re_sub(r'(\\[*)(\\w*)(\\.)(png|jpg|jpeg|gif|GIF)(\\]*)', r'\\2 ')\n",
    "\n",
    "        # Replace Currency:\n",
    "        text = re_sub(r'([$€¢£¥฿]|euro|usd|USD|EUR|Euro|Dolar|dolar|BTC|GBP|gbp|btc)', ' ')\n",
    "\n",
    "        # Replace repeated text\n",
    "        text = re_sub(r'([!?\\.\\-:]){2,}', r'\\1 ')\n",
    "\n",
    "        # Replace some - with space or leave it:\n",
    "        text = re_sub(r'(^\\-)|(\\s*)(\\-|\\^|\\+|\\*|\\–)(\\s)|(\\s)(\\-|\\^|\\+|\\*|\\–)(\\s*)', ' ')\n",
    "\n",
    "        # Replace some S.C:\n",
    "        text = re_sub(r'[(_)\\u00A9•\\|#%\\^~*\\[\\]{}\\.,:!\";\\?\\\\\\'@“”]', ' ')\n",
    "\n",
    "        # Replace - with space:\n",
    "        text = re_sub(r'-', ' ')\n",
    "\n",
    "        # Remove text ends with number\n",
    "        text = re_sub(r'(\\w{3,})([0-9])', r'\\1')\n",
    "\n",
    "        # Remove long words\n",
    "        text = ' '.join([word for word in text.split() if 15 > len(word) > 2])\n",
    "\n",
    "        # Replace NUM\n",
    "        text = re_sub(r\"[-+]?[.\\d]*[\\d]+[:,.\\d]\", ' ')\n",
    "\n",
    "        # Replace some S.C:\n",
    "        text = re_sub(r'[+=\\/]', ' ')\n",
    "\n",
    "        # Replace Single letters:\n",
    "        text = re_sub(r'(?<!\\S).(?!\\S)\\s*', '')\n",
    "\n",
    "        # Lower Text:\n",
    "        text = text.strip().lower()\n",
    "\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url = 'diamachdwhqp7pem.onion'\n",
    "with open(OnionDir.format(url), 'r', encoding ='utf-8') as fin:\n",
    "    text = fin.readlines()\n",
    "text = ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "textpreprocessor = OnionTextPreprocessor ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'марок биохаразд доб марок биохазард доб 1ое продукции случае возникновения вопроса читайте купить img кокаин classic 1gr кокаин vip гр 3ое продукции так другие утешительные призы всем кто участвовал всем lsd stamp hoffman bicycl microdot вопросам находа уточнение опта писать jabber кокаина целое множество различных наркотиков всех сортов расцветок упаковок кислоты разных кислых лсд мео дип доб пол солонки meo mipt триптамин новости http obmen либо пишите jabber дсков ена дисков фунт добро пожаловать 2ое продукции dob stamp biohazard идёт конкурс лучшие сиськи подробности тут ветка продавца микродотов мгк лсд микродотов мгк лсд руб руб mdma pill также текила ром еще вкусностей mephedron type гр амнезия амнезии спасибо что выбрали нас самый качественный товар доступным ценам идёт конкурс лучший рассказ своих ощущений подробности тут hash diamond shop moscow citi москва <email> график мск субботу приз рассказ чемодан пакета травы гр таблеток перед покупкой обязательно прочитайте правила приз сисьски состоит мест спирт спирт пилюли meo mipt пилюли meo mipt правила cocain vip classiс так тут есть помощник можете ему написать любому вопросу войти diamond support мск воскресенье выходной всем вопросам qiwi переходите страницу <email> имеет отношения оплате weed bud 5гр евро лист пожалуйста забывайте оставлять отзывы http id'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textpreprocessor.clean_onion(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
